#!pip install langchain_groq
#!pip install matplotlib


from langchain_openai import ChatOpenAI
from typing import Annotated, List, Tuple, Union
from langchain.tools import BaseTool, StructuredTool, Tool
from langchain_experimental.tools import PythonREPLTool
from langchain_community.agent_toolkits import create_sql_agent
from langchain_core.tools import tool
from langchain_groq import ChatGroq
import random
import os

from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.graph import StateGraph, END
import operator
import functools

#os.environ["OPENAI_API_KEY"] = "[token]"
#llm = ChatOpenAI(model="gpt-3.5-turbo")
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGCHAIN_API_KEY"] = "[token]"
os.environ["LANGCHAIN_PROJECT"] = "pr-teste-lang-grapth"

llm = ChatGroq(temperature=0, groq_api_key="[token]", model_name="llama-3.1-70b-versatile")

#Tools - for plotting the diagram
python_repl_tool = PythonREPLTool()
tools = [python_repl_tool]


from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_openai import ChatOpenAI

# function that returns AgentExecutor with given tool and prompt
def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                system_prompt,
            ),
            MessagesPlaceholder(variable_name="messages"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ]
    )
    agent = create_openai_tools_agent(llm, tools, prompt)
    executor = AgentExecutor(agent=agent, tools=tools)
    return executor

# agent node, funtion that we will use to call agents in our graph
def agent_node(state, agent, name):
    result = agent.invoke(state)
    return {"messages": [HumanMessage(content=result["output"], name=name)]}

def agent_sql_node(state, agent, name):
    result = agent.invoke(state["messages"][-1].content)
    return {"messages": [HumanMessage(content=result["output"], name=name)]}


from langchain_community.utilities import SQLDatabase

maria_uri = 'mysql+mysqlconnector://root:art_llama3@localhost:3306/mme'
db = SQLDatabase.from_uri(maria_uri)


def create_and_execute_sql_agent(*args, **kwargs):
    """Execute the latest SQL queries."""
    
    agent_executor = create_sql_agent(*args, **kwargs)
    result = agent_executor.execute()

    # Execute SQL query
    res = db.run(result['sql_query'], fetch="cursor").fetchall()

    # Convert result to Pandas DataFrame
    #df_columns = tool_call["args"]["df_columns"]
    df = pd.DataFrame(res)#, columns=df_columns
    #df_name = tool_call["args"]["df_name"]
    df_name = "df_name"

    # Add tool output message
    messages.append(
        RawToolMessage(
            f"Generated dataframe {df_name} with columns {df.columns}",  # What's sent to model.
            raw={df_name: df},
            #tool_call_id=tool_call["id"],
            #tool_name=tool_call["name"],
        )
    )

    return {"messages": messages}


# QueryBuild as a node
sql_agent = create_and_execute_sql_agent(llm, db=db, agent_type="tool-calling", verbose=True)
sql_node = functools.partial(agent_sql_node, agent=sql_agent, name="QueryBuild")

# Coder as a node
code_agent = create_agent(llm, [python_repl_tool], "You generate charts using matplotlib.")
code_node = functools.partial(agent_node, agent=code_agent, name="Coder")


from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

members = ["QueryBuild", "Coder"]
system_prompt = (
    "You are a supervisor tasked with managing a conversation between the"
    " following workers:  {members}. Given the following user request,"
    " respond with the worker to act next. Each worker will perform a"
    " task and respond with their results and status. When finished,"
    " respond with FINISH."
)
# It will use function calling to choose the next worker node OR finish processing.
options = ["FINISH"] + members
# openai function calling 
function_def = {
    "name": "route",
    "description": "Select the next role.",
    "parameters": {
        "title": "routeSchema",
        "type": "object",
        "properties": {
            "next": {
                "title": "Next",
                "anyOf": [
                    {"enum": options},
                ],
            }
        },
        "required": ["next"],
    },
}
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="messages"),
        (
            "system",
            "Given the conversation above, who should act next?"
            " Or should we FINISH? Select one of: {options}",
        ),
    ]
).partial(options=str(options), members=", ".join(members))


# we create the chain with llm binded with routing function + system_prompt
supervisor_chain = (
    prompt
    | llm.bind_functions(functions=[function_def], function_call="route")
    | JsonOutputFunctionsParser()
)


# defining the AgentState that holds messages and where to go next
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]
    # The 'next' field indicates where to route to next
    next: str

# defining the StateGraph
workflow = StateGraph(AgentState)

# agents as a node, supervisor_chain as a node 
workflow.add_node("QueryBuild", sql_node)
workflow.add_node("Coder", code_node)
workflow.add_node("supervisor", supervisor_chain)

# when agents are done with the task, next one should be supervisor ALWAYS
workflow.add_edge("QueryBuild", "supervisor") 
workflow.add_edge("Coder", "supervisor")

# Supervisor decides the "next" field in the graph state, 
# which routes to a node or finishes. (Remember the special node END above)
workflow.add_conditional_edges(
                    "supervisor", 
                    lambda x: x["next"], 
                    {
                       "QueryBuild": "QueryBuild",
                       "Coder": "Coder",
                       "FINISH": END 
                    })

# starting point should be supervisor
workflow.set_entry_point("supervisor")


graph = workflow.compile()


print(graph.get_graph().draw_ascii())


final_state = graph.invoke(
    {"messages": [HumanMessage(content="Qual foi o top 4 estados que tiveram o maior consumo em MWh total? Gere um gráfico de barras com o estado e o total MWh")]}
)


for s in graph.stream(
    {
        "messages": [
            HumanMessage(content="Qual foi o top 4 estados que tiveram o maior consumo em MWh total? Gere um gráfico de barras com o estado e o total MWh")
        ]
    }, config={"recursion_limit": 6}
):
    if "__end__" not in s:
        print(s)
        print("----")



